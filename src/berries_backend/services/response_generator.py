"""
Service to generate the final response to the user based on the query and retrieved context.
"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser


class ResponseGenerator:
    """
    Generates a user-facing response using an LLM, grounded in the provided context.
    """

    def __init__(self, google_api_key: str):
        """
        Initialize the ResponseGenerator.

        Args:
            google_api_key: Google API key for the response generation LLM.
        """
        # Initialize LLM instance for response generation
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=google_api_key,
            temperature=0.5,
        )

        # Define the prompt template for final answer generation
        template = """You are a helpful financial assistant. Your task is to answer the user's query based *only* on the provided context.

        Follow these instructions strictly:
        1. Analyze the user's query: "{user_query}"
        2. Review the following context data carefully:
        <context>
        {context_string}
        </context>
        3. Formulate a clear and concise answer to the user's query using *only* the information present in the <context>.
        4. Do *not* make up information or use any external knowledge not explicitly provided in the context.
        5. If the context does not contain the information needed to answer the query, state that clearly. For example, say "Based on the provided information, I cannot answer [specific part of the query]" or "Sorry, I don't have the specific data required ([e.g., SEC filing for that year], [specific stock data]) to answer your question."
        6. Do not mention the word "context" in your final response to the user.
        7. Answer *only* the user's query. Do not add extra information not requested.

        User Query: {user_query}

        Answer:"""
        self.prompt = ChatPromptTemplate.from_template(template)

        # Create the response generation chain
        self.chain = self.prompt | self.llm | StrOutputParser()


    def generate_response(self, user_query: str, context_string: str) -> str:
        """
        Generates the final answer based on the query and retrieved context.

        Args:
            user_query: The original user query.
            context_string: The context string generated by the DataRetriever.

        Returns:
            The final, user-facing response string.
        """
        print("\nüß† Generating final response...")
        try:
            # Invoke the chain with the query and context
            response = self.chain.invoke({
                "user_query": user_query,
                "context_string": context_string
            })
            print("‚úÖ Response generated successfully.")
            return response

        except Exception as e:
            print(f"‚ùå Error generating final response: {e}")
            # Fallback response in case of generation error
            return "Sorry, I encountered an error while trying to generate a response."
